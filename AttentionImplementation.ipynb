{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM+479CiJgk/P4n11KXdE0n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rula-Islait/Research/blob/main/AttentionImplementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "metadata": {
        "id": "4_0KJ-6ObeDl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EImPdo2VZw5z"
      },
      "outputs": [],
      "source": [
        "class ReverseAttention(layers.Layer):\n",
        "    def __init__(self, num_heads=8, key_dim=64):\n",
        "        super(ReverseAttention, self).__init__()\n",
        "        self.mha = layers.MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        attention_output = self.mha(inputs, inputs)\n",
        "        return tf.sigmoid(1 - attention_output) + tf.math.tanh(attention_output)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DynamicAttention(layers.Layer):\n",
        "    def __init__(self, channels, num_heads=8, key_dim=64):\n",
        "        super(DynamicAttention, self).__init__()\n",
        "        self.channels = channels\n",
        "        self.num_heads = num_heads\n",
        "        self.key_dim = key_dim\n",
        "        self.mha = layers.MultiHeadAttention(num_heads=self.num_heads, key_dim=self.key_dim)\n",
        "        self.batch_norm = layers.BatchNormalization()\n",
        "        self.dropout = layers.Dropout(0.3)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        attention_output = self.mha(inputs, inputs)\n",
        "        attention_output = self.batch_norm(attention_output)\n",
        "        attention_output = self.dropout(attention_output)\n",
        "        return inputs * attention_output + inputs"
      ],
      "metadata": {
        "id": "BeuQ9oYnbkEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SequentialAttention(layers.Layer):\n",
        "    def __init__(self, channels, num_heads=8, key_dim=64):\n",
        "        super(SequentialAttention, self).__init__()\n",
        "        self.channels = channels\n",
        "        self.num_heads = num_heads\n",
        "        self.key_dim = key_dim\n",
        "        self.mha = layers.MultiHeadAttention(num_heads=self.num_heads, key_dim=self.key_dim)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = layers.Conv2D(self.channels, (3, 3), padding='same', activation='relu')(inputs)\n",
        "        attention_output = self.mha(x, x)\n",
        "        attention_map = layers.Conv2D(self.channels, (3, 3), padding='same', activation='sigmoid')(attention_output)\n",
        "        return inputs + (inputs * attention_map)"
      ],
      "metadata": {
        "id": "xgJwUTQsbnzQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DenseAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, channels):\n",
        "        super(DenseAttention, self).__init__()\n",
        "        self.channels = channels\n",
        "        self.mha = tf.keras.layers.MultiHeadAttention(num_heads=4, key_dim=channels)\n",
        "        self.pool = tf.keras.layers.GlobalAveragePooling2D()\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.query_dim = input_shape[-1]\n",
        "\n",
        "    def call(self, inputs):\n",
        "        pooled_inputs = self.pool(inputs)\n",
        "        pooled_inputs = tf.expand_dims(pooled_inputs, 1)\n",
        "        attention_output = self.mha(pooled_inputs, pooled_inputs)\n",
        "        return inputs * tf.expand_dims(attention_output, 1) + inputs\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return tf.TensorShape([input_shape[0], input_shape[1], input_shape[2], self.channels])"
      ],
      "metadata": {
        "id": "cfq51zJBb0iO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DXSyO9j6Wwxr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}